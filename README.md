# PocketGuide

**Offline-capable travel assistant LLM with evaluation-first design**

PocketGuide is a specialized language model assistant for travelers, providing reliable guidance on visa requirements, border regulations, local customs, budgeting, and itinerary planningâ€”without internet connectivity. Built with rigorous evaluation, deterministic outputs, and robust error handling.

## Key Features

- **Offline-first**: No external API dependencies
- **Evaluation-driven**: Comprehensive benchmark suites for development
- **Structured responses**: Consistent output format with required sections
- **Deterministic**: Reproducible outputs for testing and reliability
- **Production-ready**: Clean architecture with stable commands

## Quick Start

### Prerequisites
- Python 3.11+
- Make

### Setup
```bash
git clone <repository-url>
cd pocket-guide
make env
source .venv/bin/activate
make test

# Set up API keys for data generation
cp .env.example .env
# Edit .env and add your OPENROUTER_API_KEY
```

### Basic Usage
```bash
make run          # Run inference with default prompt
make eval         # Run evaluation on benchmark suites
make lint         # Code quality checks
make data         # Generate synthetic training data (requires API key)
```

### Teacher Model (Synthetic Data Generation)
```bash
# Test in dry-run mode (no API calls)
python -m pocketguide.teachers.smoke --dry-run

# Real API calls (uses .env for API key)
python -m pocketguide.teachers.smoke --real --prompt "Best cafes in Rome?"
```

See [docs/environment-setup.md](docs/environment-setup.md) for API key configuration.

## Usage

### CLI Inference
```bash
python -m pocketguide.inference.cli --prompt "What are visa requirements for Japan?"
python -m pocketguide.inference.cli --prompt "Budget for Thailand trip" --format json
```

### Evaluation
```bash
# Run full benchmark suite
make eval

# Run specific suite
python -m pocketguide.eval.benchmark --suite_dir data/benchmarks/v0

# Smoke test with real model
python -m pocketguide.eval.benchmark --smoke_infer --prompt "Explain Japan visa policy"
```

Results saved to `runs/eval/<run_id>/` with outputs, metrics, and provenance metadata.

### Reports
Generate human-readable evaluation reports:
```bash
python -m pocketguide.eval.report_base_v0 --run_dir runs/eval/<timestamp>
```

Includes metrics summary, failure analysis, and curated examples.

## Project Structure

```
pocket-guide/
â”œâ”€â”€ src/pocketguide/          # Core package
â”‚   â”œâ”€â”€ inference/           # Model inference and CLI
â”‚   â”œâ”€â”€ eval/                # Evaluation framework (benchmark, metrics, parsing, reporting)
â”‚   â”œâ”€â”€ teachers/            # Teacher models for synthetic data generation
â”‚   â”œâ”€â”€ data_generation/     # Prompt planning and dataset specs
â”‚   â”œâ”€â”€ data/schemas/        # JSON schemas (v0 envelope, v1 payloads)
â”‚   â””â”€â”€ utils/               # Utilities (rate limiting, etc.)
â”œâ”€â”€ configs/                 # Configuration files (eval, teacher)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ benchmarks/v0/       # Benchmark suites (72 examples)
â”‚   â”œâ”€â”€ prompts/teacher/v1/  # Teacher prompt templates
â”‚   â””â”€â”€ specs/               # Dataset specifications
â”œâ”€â”€ tests/                   # Test suite (156 tests)
â”œâ”€â”€ runs/                    # Evaluation outputs
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ .env.example             # Environment variable template
â”œâ”€â”€ Makefile                 # Build commands
â””â”€â”€ pyproject.toml           # Package config
```

## Development Roadmap

```
M0: Foundation    M1: Baseline      M2: Contracts     M3: Data Engine   M4-9: Refinement
âœ… Complete      âœ… Complete       âœ… Complete       ğŸ”„ In Progress    â³ Planned
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

| Milestone | Status | Goal | Key Deliverables |
|-----------|--------|------|------------------|
| **M0** | âœ… | Clean foundation | Repo structure, stable commands, end-to-end pipeline |
| **M1** | âœ… | Baseline eval | Base model report, reference benchmarks (72 examples) |
| **M2** | âœ… | Response contracts | Envelope schema (v0), payload schemas (v1), validation engine |
| **M3** | ğŸ”„ | Synthetic data | Teacher provider, prompt templates, deterministic generation |
| **M4** | â³ | Data quality | Deduplication, balancing, clean splits |
| **M5** | â³ | Model tuning | LoRA/QLoRA fine-tuning, experiment tracking |
| **M6** | â³ | Rigorous eval | Base vs adapted comparison, metrics + examples |
| **M7** | â³ | Iteration | Evidence-driven improvements, retrain & re-eval |
| **M8** | â³ | Deployment | Quantization, offline packaging, resource docs |
| **M9** | â³ | Portfolio | Polish, demo, limitations, safety |

### Current Focus: Milestone 3 â€” Synthetic Data Engine

**What:** Building a teacher-driven pipeline to generate high-quality travel instruction examples with proper structure, uncertainty handling, and verification guidance.

**Completed:**
- âœ… Prompt templates (v1) for 4 payload types
- âœ… Dataset spec (120 examples, 7 categories, 3 difficulty levels)
- âœ… OpenRouter backend with cost-controlled fallback (2 free â†’ 1 paid)
- âœ… Rate limiting, exponential backoff, retry logic, error typing
- âœ… Environment-based API key management (.env + python-dotenv)
- âœ… 156 tests passing

**In Progress:**
- ğŸ”„ Batch generation CLI (Lesson 3.3)
- ğŸ”„ Response validation & quality checks
- ğŸ”„ Dataset versioning

## Development

### Testing
```bash
make test                    # Run all tests
pytest tests/ --cov=pocketguide  # With coverage
```

### Code Quality
```bash
make lint                    # Check linting
make format                  # Auto-format
```

### Adding Benchmarks
Create JSONL files in `data/benchmarks/v0/`:
```jsonl
{"id": "example_001", "prompt": "Travel question here"}
```

Run evaluation:
```bash
python -m pocketguide.eval.benchmark --suite_dir data/benchmarks/v0
```

## Contributing

- Type hints and Google-style docstrings
- Comprehensive test coverage
- Ruff for linting and formatting

## License

*(To be determined)*
